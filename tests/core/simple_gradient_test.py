#!/usr/bin/env python3
"""
Simple test to verify the gradient checking functionality works.
This tests basic PyTorch gradient computation using numerical differentiation.
"""

import torch
import numpy as np
import random
import math


def test_gradient_check_simple():
    """Simple gradient check for a basic function."""
    # Set deterministic seeds for reproducible tests
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(42)
        torch.cuda.manual_seed_all(42)
    if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        torch.mps.manual_seed(42)

    # Set deterministic algorithms for reproducibility
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    # Determine device: prefer CUDA > MPS > CPU
    if torch.cuda.is_available():
        device = torch.device('cuda')
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device('mps')
    else:
        device = torch.device('cpu')
    

    # MPS doesn't support float64, use float32 instead
    dtype = torch.float32 if device.type == 'mps' else torch.float64
    
    print(f"Using device: {device}, dtype: {dtype}")
    
    # Simple quadratic function: f(x) = sum(x^2)
    x = torch.randn(5, device=device, dtype=dtype, requires_grad=True)
    
    def func():
        return torch.sum(x ** 2)
    
    # Analytical gradient
    output = func()
    output.backward()
    analytical_grad = x.grad.clone() if x.grad is not None else torch.zeros_like(x)
    
    # Numerical gradient
    eps = 1e-8
    if x.grad is not None:
        x.grad.zero_()
    numerical_grad = torch.zeros_like(x)
    
    for i in range(x.numel()):
        # Positive perturbation
        x.data[i] += eps
        f_plus = func()
        
        # Negative perturbation  
        x.data[i] -= 2 * eps
        f_minus = func()
        
        # Restore
        x.data[i] += eps
        
        # Compute numerical gradient
        numerical_grad[i] = (f_plus - f_minus) / (2 * eps)
        
    
    # Compare
    error = torch.max(torch.abs(analytical_grad - numerical_grad))
    rel_error = error / torch.max(torch.abs(analytical_grad))
    
    print(f"Analytical gradient: {analytical_grad}")
    print(f"Numerical gradient:  {numerical_grad}")
    print(f"Max absolute error:  {error:.2e}")
    print(f"Max relative error:  {rel_error:.2e}")
    
    # Check if gradient is correct (should be 2*x)
    expected_grad = 2 * x.data
    expected_error = torch.max(torch.abs(analytical_grad - expected_grad))
    print(f"Expected gradient:   {expected_grad}")
    print(f"Error vs expected:   {expected_error:.2e}")
    
    assert error < 1e-6, f"Gradient check FAILED: error={error:.2e} >= 1e-6"
    print("âœ“ Gradient check PASSED")


if __name__ == "__main__":
    # Allow running as standalone script
    test_gradient_check_simple()
